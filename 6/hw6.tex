\documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array,geometry,enumitem,amsmath}
\setlength{\parskip}{1ex} %--skip lines between paragraphs
\setlength{\parindent}{0pt} %--don't indent paragraphs

%-- Commands for header
\renewcommand{\title}[1]{\textbf{#1}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-0.5cm]}

%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{Digital Signal Processing - Assignment 6}
\line
\leftright{\today}{Stephanie Lund (2555914)\\Aljoscha Dietrich (2557976)} %-- left and right positions in the header

\section*{Exercise 1}

\subsection*{1.1}
This is the Method of Least Squares, which can be solved as follows. First, find the partial derivatives by $a$ and $b$, and set them to $0$:

\begin{align*}
\frac{\partial \epsilon}{\partial a} &= \sum_{i=1}^N 2x_i(ax_i + b - y_i) = 0 \\\\
\frac{\partial \epsilon}{\partial b} &= \sum_{i=1}^N 2(ax_i + b - y_i) = 0
\end{align*}

Next, rewrite them as a series of linear equations:

\begin{align*}
\left[ \sum_{i=1}^N x_i^2\right]a + \left[ \sum_{i=1}^N x_i\right] b &= \sum_{i=1}^N x_iy_i  \\\\
\left[ \sum_{i=1}^N x_i\right]a + \left[ \sum_{i=1}^N 1\right] b &=  \sum_{i=1}^N y_i
\end{align*}

Then solve the equations for $a$ and $b$:

\begin{align*}
a =& \frac{N \sum_{i=1}^N(x_iy_i) - \sum_{i=1}^Nx_i\sum_{i=1}^Ny_i}{N\sum_{i=1}^Nx_i^2 - \left(\sum_{i=1}^Nx_i\right)^2}
\\ \\
b =& \frac{\sum_{i=1}^Ny_i - \left(\sum_{i=1}^Nx_i\right)a}{N}
\end{align*}

\subsection*{1.2}
The results from the attached Matlab script hw12.m are $a = -0.3613$ and $b = 1.1576$.

\section*{Exercise 2}
TODO

\end{document}